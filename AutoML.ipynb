{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Sample Data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: [\n\u001b[1;32m     18\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mI love machine learning\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mDeep learning is amazing\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m ,\u001b[39m0\u001b[39m  \u001b[39m# Additional mixed sentiment examples\u001b[39;00m\n\u001b[1;32m     41\u001b[0m ]}\n\u001b[0;32m---> 43\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data)\n\u001b[1;32m     45\u001b[0m \u001b[39m# Download NLTK data\u001b[39;00m\n\u001b[1;32m     46\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_automl/lib/python3.8/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_automl/lib/python3.8/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_automl/lib/python3.8/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_automl/lib/python3.8/site-packages/pandas/core/internals/construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    658\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Sample Data\n",
    "data = {'text': [\n",
    "    'I love machine learning', \n",
    "    'Deep learning is amazing', \n",
    "    'Natural language processing is fun', \n",
    "    'I enjoy coding in Python', \n",
    "    'Data science is a great field',\n",
    "    'This product is of very poor quality. It broke after just one use. I would not recommend it to anyone.',\n",
    "    'The movie was a total waste of time. The plot was boring and the acting was terrible.',\n",
    "    'The food was awful and the service was even worse. I will never eat here again.',\n",
    "    'I had a terrible experience at the hotel. The room was dirty, and the amenities were not as described.',\n",
    "    'I feel undervalued and overworked in this job. The management does not care about employee well-being.',\n",
    "    'I am feeling very unhappy and stressed out with how things are going in my life right now.',\n",
    "    'The event was well-organized and everyone had a great time.',\n",
    "    'I absolutely love this new app! It makes my life so much easier.',\n",
    "    'The customer service was excellent. They were very helpful and resolved my issue quickly.',\n",
    "    'The book was inspiring and beautifully written. I couldn\\'t put it down.',\n",
    "    'The concert exceeded my expectations. The performers were incredible.',\n",
    "    'The new restaurant in town is fantastic. The food is delicious and the ambiance is perfect.',\n",
    "    'I hate u'\n",
    "],\n",
    "'label': [\n",
    "    1, 1, 1, 1, 1,  # Positive examples\n",
    "    0, 0, 0, 0, 0,  # Negative examples\n",
    "    0, 1, 1, 1, 1, 1, 1 ,0  # Additional mixed sentiment examples\n",
    "]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform text data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define model type\n",
    "    model_type = trial.suggest_categorical('model_type', ['logistic_regression', 'svm', 'naive_bayes'])\n",
    "    \n",
    "    if model_type == 'logistic_regression':\n",
    "        C = trial.suggest_float('C', 1e-5, 1e2, log=True)\n",
    "        model = LogisticRegression(C=C, max_iter=1000)\n",
    "    elif model_type == 'svm':\n",
    "        C = trial.suggest_float('C', 1e-5, 1e2, log=True)\n",
    "        model = SVC(C=C)\n",
    "    else:\n",
    "        alpha = trial.suggest_float('alpha', 1e-5, 1e0, log=True)\n",
    "        model = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Fit model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best model\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters: \", best_params)\n",
    "\n",
    "# Extract best parameters\n",
    "model_type = best_params['model_type']\n",
    "\n",
    "if model_type == 'logistic_regression':\n",
    "    best_model = LogisticRegression(C=best_params['C'], max_iter=1000)\n",
    "elif model_type == 'svm':\n",
    "    best_model = SVC(C=best_params['C'])\n",
    "else:\n",
    "    best_model = MultinomialNB(alpha=best_params['alpha'])\n",
    "\n",
    "# Create a pipeline\n",
    "final_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "# Train final model\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final model accuracy: \", accuracy)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(final_pipeline, 'nlp_automl_model.pkl')\n",
    "\n",
    "# Load the model (for inference)\n",
    "loaded_model = joblib.load('nlp_automl_model.pkl')\n",
    "\n",
    "# Example prediction\n",
    "example_text = [\"I love studying data science\"]\n",
    "example_cleaned = [preprocess(text) for text in example_text]\n",
    "prediction = loaded_model.predict(example_cleaned)\n",
    "print(\"Prediction: \", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_automl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
